{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy import linalg\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config_categ\n",
    "from utils.reproducibility import set_seed\n",
    "from agents.DQN import DQNAgent\n",
    "from agents.LaplaceDQN import LaplaceDQNAgent\n",
    "from envs.four_state_mdp import Simple4MDP\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = 'CartPole-v1'\n",
    "log_dir = 'logs/tmp'\n",
    "\n",
    "env = gym.make(game, render_mode=None)\n",
    "config = config_categ[game](env, log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.num_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "5\n",
      "500\n",
      "2\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(config.BATCH_SIZE)\n",
    "print(config.num_gamma)\n",
    "print(config.num_sensitivities)\n",
    "print(config.action_dim)\n",
    "print(config.num_gamma_to_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 100, 500])\n"
     ]
    }
   ],
   "source": [
    "batch_size, num_actions, num_gamma_to_tau, num_sensitivities = 1, config.action_dim, config.num_gamma_to_tau, config.num_sensitivities\n",
    "Q_gamma = torch.randn(size=(batch_size, num_actions, num_gamma_to_tau, num_sensitivities))\n",
    "# Q_gamma = np.random.normal(size=(batch_size, num_actions, num_gamma_to_tau, num_sensitivities))\n",
    "print(Q_gamma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Inverse_Laplace import SVD_approximation_inverse_Laplace, SVD_approximation_inverse_Laplace_iterative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alpayozkan/Desktop/projects/RL/distrl_proj/utils/Inverse_Laplace.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gammas_to_tau_tensor = torch.tensor(gammas_to_tau)\n"
     ]
    }
   ],
   "source": [
    "tau_space = SVD_approximation_inverse_Laplace(config, Q_gamma,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_space_iter = SVD_approximation_inverse_Laplace_iterative(config, Q_gamma.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(tau_space_iter, tau_space, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1269)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.abs(tau_space_iter-tau_space).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 200, 498])\n"
     ]
    }
   ],
   "source": [
    "print(tau_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def SVD_approximation_inverse_Laplace(config, Q_gamma):\n",
    "\"\"\"\n",
    "SVD-based approximation of the inverse Laplace transform \n",
    "Input:\n",
    "config: configuration object from config.py\n",
    "Q_gamma: Q-values for different gamma values and sensitivities, \n",
    "            shape (batch_size, num_gamma, num_sensitivities)\n",
    "\"\"\"\n",
    "\n",
    "alpha_reg = config.alpha_reg\n",
    "K = config.K\n",
    "delta_t = config.delta_t\n",
    "\n",
    "#batch_size = config.BATCH_SIZE # NOTE maybe not at the beginning of the training - then assume change in time horizon takes place after that\n",
    "batch_size = Q_gamma.shape[0]\n",
    "num_sensitivities = config.num_sensitivities\n",
    "num_actions = config.action_dim\n",
    "num_gamma_to_tau = config.num_gamma_to_tau\n",
    "gamma_to_tau_min = config.gamma_to_tau_min\n",
    "gamma_to_tau_max = config.gamma_to_tau_max\n",
    "start = 1 / np.log(gamma_to_tau_min) \n",
    "end = 1 / np.log(gamma_to_tau_max)   \n",
    "gammas_to_tau = torch.exp(torch.true_divide(1, torch.linspace(start, end, num_gamma_to_tau)))\n",
    "\n",
    "assert Q_gamma.shape == (batch_size, num_actions, num_gamma_to_tau, num_sensitivities), \"Q_gamma shape does not match (num_gamma, num_sensitivities)\"\n",
    "# finish extending to actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define matrix F:\n",
    "F=torch.zeros((len(gammas_to_tau),K))\n",
    "for i_g in range(0,len(gammas_to_tau)):\n",
    "    for i_t in range(0,K):\n",
    "        F[i_g,i_t]=gammas_to_tau[i_g]**(i_t*delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dp/4fr32ggj4tb41_86_5pj1l8h0000gn/T/ipykernel_92036/864225259.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  gammas_to_tau_tensor = torch.tensor(gammas_to_tau)\n"
     ]
    }
   ],
   "source": [
    "# MATRIX OPERATIONS\n",
    "\n",
    "#define matrix F:\n",
    "F_parallel = torch.zeros((len(gammas_to_tau), K))\n",
    "gammas_to_tau_tensor = torch.tensor(gammas_to_tau)\n",
    "delta_t_tensor = torch.tensor(delta_t)\n",
    "\n",
    "# Perform matrix operations instead of loops\n",
    "F_parallel = gammas_to_tau_tensor.unsqueeze(1).pow(torch.arange(K).float() * delta_t_tensor)\n",
    "\n",
    "# F_numpy = F.numpy()  # Convert the tensor back to a numpy array if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(F_parallel, F, atol=1e-21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(100,)\n",
      "(200, 200)\n",
      "torch.Size([100, 200])\n"
     ]
    }
   ],
   "source": [
    "U_arr, lam_arr, V_arr = linalg.svd(F)\n",
    "\n",
    "print(U_arr.shape)\n",
    "print(lam_arr.shape)\n",
    "print(V_arr.shape)\n",
    "print(F.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([100])\n",
      "torch.Size([200, 200])\n"
     ]
    }
   ],
   "source": [
    "# U, lam, V = linalg.svd(F) #SVD decomposition of F\n",
    "U, lam, V = torch.linalg.svd(F) #SVD decomposition of F\n",
    "print(U.shape)\n",
    "print(lam.shape)\n",
    "print(V.shape)\n",
    "\n",
    "# set up gamma-space:\n",
    "Z=Q_gamma[:,:,:,0:-2]-Q_gamma[:,:,:,1:-1]\n",
    "\n",
    "# smooth gamma-space (it might not be necessary, it helps if the input is *very* noisy):\n",
    "# for h in range(0,num_h-2):\n",
    "#     Z[:,h]=savgol_filter(Z[:,h], 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Linearly recover tau-space from eigenspace of F:\n",
    "tau_space=torch.zeros((batch_size, num_actions, K, num_sensitivities-2))\n",
    "# do for several batches and actions-> parallelize TODO\n",
    "for batch in range(batch_size):\n",
    "    for act in range(num_actions):\n",
    "        for h in range(0,num_sensitivities-2):\n",
    "            term=torch.zeros((1,K))\n",
    "            for i in range(0,len(lam)):\n",
    "                fi=lam[i]**2/(alpha_reg**2+lam[i]**2)\n",
    "                new=fi*(((U[:,i]@Z[batch,act,:,h])*V[i,:] )/lam[i])\n",
    "                term=term+new\n",
    "            tau_space[batch,act,:,h]=term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(V[i,:].shape)\n",
    "print((U[:,i]@Z[batch,act,:,h]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 200])\n",
      "torch.Size([200])\n",
      "torch.Size([1, 2, 200, 498])\n"
     ]
    }
   ],
   "source": [
    "print(term.shape)\n",
    "print(new.shape)\n",
    "print(tau_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Linearly recover tau-space from eigenspace of F:\n",
    "tau_space_paralell=torch.zeros((batch_size, num_actions, K, num_sensitivities-2))\n",
    "# do for several batches and actions-> parallelize TODO\n",
    "for batch in range(batch_size):\n",
    "    for act in range(num_actions):\n",
    "        for h in range(0,num_sensitivities-2):\n",
    "            term=torch.zeros((1,K))\n",
    "            for i in range(0,len(lam)):\n",
    "                fi=lam[i]**2/(alpha_reg**2+lam[i]**2)\n",
    "                new=fi*(((U[:,i] @ Z[batch,act,:,h]) * V[i,:] )/lam[i])\n",
    "                term=term+new\n",
    "            \n",
    "            tau_space_paralell[batch,act,:,h]=term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Compute fi values\n",
    "fi = lam**2 / (alpha_reg**2 + lam**2)\n",
    "fi = fi.reshape(1,1,-1,1)\n",
    "print(fi.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = lam**2 / (alpha_reg**2 + lam**2)\n",
    "# Expand tensors for broadcasting\n",
    "fi = fi.reshape(1, 1, -1, 1)  # Shape: (1, 1, len(lam), 1)\n",
    "U_expanded = U.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, K, len(lam))\n",
    "V_expanded = V.unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, len(lam), K)\n",
    "\n",
    "tau_space_paralell = torch.zeros((batch_size, num_actions, K, num_sensitivities - 2), device=device)\n",
    "for h in range(num_sensitivities - 2):\n",
    "    Z_expanded = Z[:, :, :, h].unsqueeze(2)\n",
    "\n",
    "    tmp = (Z_expanded @ U_expanded).permute(0,1,3,2)\n",
    "    V_lam = V_expanded[:,:,:len(lam),:] \n",
    "    term = (fi * (tmp * V_lam) / lam.reshape(1, 1, -1, 1)).sum(dim=2)\n",
    "    tau_space_paralell[:, :, :, h] = term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 200, 498])\n",
      "torch.Size([1, 2, 200, 498])\n"
     ]
    }
   ],
   "source": [
    "print(tau_space_paralell.shape)\n",
    "print(tau_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(3,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0100, 0.4406, 0.6377, 0.7334, 0.7894, 0.8260, 0.8518, 0.8709, 0.8857,\n",
       "        0.8975, 0.9070, 0.9149, 0.9216, 0.9273, 0.9323, 0.9366, 0.9404, 0.9437,\n",
       "        0.9467, 0.9494, 0.9519, 0.9541, 0.9561, 0.9580, 0.9597, 0.9612, 0.9627,\n",
       "        0.9640, 0.9653, 0.9664, 0.9675, 0.9686, 0.9695, 0.9704, 0.9713, 0.9721,\n",
       "        0.9728, 0.9736, 0.9742, 0.9749, 0.9755, 0.9761, 0.9767, 0.9772, 0.9777,\n",
       "        0.9782, 0.9787, 0.9791, 0.9795, 0.9799, 0.9803, 0.9807, 0.9811, 0.9814,\n",
       "        0.9818, 0.9821, 0.9824, 0.9827, 0.9830, 0.9833, 0.9836, 0.9838, 0.9841,\n",
       "        0.9844, 0.9846, 0.9848, 0.9851, 0.9853, 0.9855, 0.9857, 0.9859, 0.9861,\n",
       "        0.9863, 0.9865, 0.9867, 0.9868, 0.9870, 0.9872, 0.9873, 0.9875, 0.9876,\n",
       "        0.9878, 0.9879, 0.9881, 0.9882, 0.9884, 0.9885, 0.9886, 0.9888, 0.9889,\n",
       "        0.9890, 0.9891, 0.9892, 0.9894, 0.9895, 0.9896, 0.9897, 0.9898, 0.9899,\n",
       "        0.9900])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(torch.true_divide(1, torch.linspace(start, end, num_gamma_to_tau))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sum(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[183], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtau_space_paralell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau_space\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: sum(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "torch.sum(~torch.allclose(tau_space_paralell, tau_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(tau_space_paralell, tau_space, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5887e-01,  1.5192e+00, -2.3931e-01, -4.9895e-01, -1.2672e+00,\n",
       "         1.2690e+00,  5.5180e-01, -7.0137e-01, -4.0276e-01,  1.1063e+00,\n",
       "        -1.7431e+00,  2.2606e-01,  1.8534e-02,  1.1908e+00,  3.4091e-01,\n",
       "        -1.1821e+00, -7.8780e-01,  3.5548e-01,  1.0009e+00,  1.0632e-01,\n",
       "        -3.2831e-01,  6.6244e-01, -1.4574e-01, -1.0425e+00, -5.1515e-01,\n",
       "         2.0787e+00,  6.6962e-02, -1.4101e+00,  3.5819e-01, -2.0816e-01,\n",
       "        -1.3312e+00, -4.5513e-01,  4.4870e-01,  4.9707e-01,  2.5160e-01,\n",
       "         1.5117e+00, -2.7640e+00,  1.7142e+00,  3.4535e-01, -9.5751e-01,\n",
       "         2.1636e+00, -1.3937e+00,  3.3830e-01, -1.8516e-01,  2.8886e-01,\n",
       "         2.2719e-01, -6.0110e-01, -3.7749e-02,  7.9825e-01, -2.2316e-01,\n",
       "         5.4351e-01, -2.3285e+00,  1.2120e+00,  1.3499e+00, -1.7468e+00,\n",
       "        -1.8926e-01, -4.4488e-01,  1.6370e+00, -1.1455e+00,  5.6134e-02,\n",
       "         6.1739e-01,  2.5897e-01, -1.1239e-02, -2.6212e-01,  3.1307e-02,\n",
       "        -3.6046e-01,  1.9612e+00, -9.9431e-01, -1.8707e+00,  3.3488e-01,\n",
       "         3.7976e-01, -3.2490e-01,  5.6010e-01, -5.0206e-01,  1.9878e-01,\n",
       "         1.5906e+00, -9.2202e-01, -5.4628e-01, -3.8638e-01,  6.7701e-01,\n",
       "        -1.8551e+00,  8.1489e-01,  1.5125e+00, -2.0028e-01, -7.0448e-02,\n",
       "        -1.5750e-01,  2.5795e+00, -2.2891e+00, -1.0721e+00,  6.8359e-01,\n",
       "        -1.0705e+00,  8.0446e-01, -5.0054e-01,  1.9562e+00, -3.0376e+00,\n",
       "         1.0187e+00,  1.5471e-01,  6.2300e-02,  1.0780e+00, -1.3124e+00,\n",
       "         1.0291e+00, -1.2619e-01,  2.3308e-01,  4.6691e-01, -9.7600e-01,\n",
       "         8.1542e-01, -9.3589e-01,  6.1739e-01, -2.1243e-02, -1.3588e+00,\n",
       "         1.1577e+00, -3.1527e-02, -1.1352e+00,  3.9337e-01,  2.0088e-01,\n",
       "         1.7510e+00, -1.8462e+00,  9.0798e-01, -7.4378e-01,  4.0243e-01,\n",
       "         7.0501e-02, -7.5712e-01,  1.3882e+00, -1.2942e+00, -2.1508e-01,\n",
       "         1.3998e+00, -1.5090e+00, -4.2789e-01,  1.0214e+00,  3.1513e-01,\n",
       "        -1.0907e+00,  7.8192e-01,  1.7878e+00, -1.8263e+00,  2.4576e-01,\n",
       "         1.4453e+00, -2.2494e-01, -8.0719e-01,  6.9224e-01, -1.4605e-01,\n",
       "        -2.2172e-01, -4.7186e-01,  1.9929e-01, -5.6445e-01,  4.6436e-02,\n",
       "        -5.7804e-01,  7.0168e-01, -2.9422e-01, -6.5954e-01,  1.3863e+00,\n",
       "        -8.0903e-01, -9.0469e-01,  1.3607e+00,  5.1854e-01, -3.2137e-01,\n",
       "        -1.2875e+00,  1.5410e+00, -5.7165e-01,  1.2456e-01,  1.3840e+00,\n",
       "        -1.9995e+00,  3.5822e-01,  1.0151e+00, -1.8127e-01, -4.7247e-01,\n",
       "        -1.1546e+00,  6.5900e-01,  1.0720e+00, -6.0584e-01,  1.2438e+00,\n",
       "        -2.1915e+00,  1.2547e+00, -2.4979e-01, -4.7562e-02,  6.0863e-01,\n",
       "        -3.7850e-01,  2.4582e-01, -1.8402e-01, -4.0823e-02, -1.2630e+00,\n",
       "         1.7934e+00, -4.5852e-01, -6.1057e-01,  7.6922e-01,  1.0000e-01,\n",
       "        -8.7129e-01,  1.2575e-01, -1.6938e-02,  8.2695e-02,  4.7032e-01,\n",
       "        -1.0233e+00, -9.6425e-01,  1.4951e+00, -1.6616e+00,  1.0369e+00,\n",
       "         1.5422e+00, -3.6988e-01, -1.8290e+00,  1.2169e+00, -1.6221e+00,\n",
       "         1.0629e+00,  5.7622e-01,  1.0808e+00, -1.0598e+00, -3.7464e-01,\n",
       "        -8.0165e-01,  3.6734e-01,  1.0252e+00,  6.2546e-02, -1.4832e+00,\n",
       "         1.4258e+00,  6.2759e-01, -2.0650e+00,  2.3637e+00, -6.2475e-01,\n",
       "         7.2606e-02,  1.0529e-01, -1.0836e+00, -8.1577e-01, -4.0202e-01,\n",
       "         1.6438e+00, -5.4273e-01,  4.8675e-01,  1.0961e+00, -1.3784e+00,\n",
       "         9.2545e-01,  9.4935e-01, -2.2071e+00, -3.7920e-01,  1.8237e+00,\n",
       "         5.7227e-02, -9.1851e-01, -1.1431e+00,  5.8274e-01,  1.7736e-01,\n",
       "         4.6752e-01, -2.5604e-01,  1.3582e+00, -4.5342e-01,  3.0559e-01,\n",
       "        -7.5229e-01, -1.2753e+00,  1.5970e+00,  5.4402e-01, -8.4945e-01,\n",
       "        -6.2085e-01,  1.5971e-01,  9.3169e-01, -3.7399e-02,  2.2801e-01,\n",
       "        -1.6180e+00,  8.0465e-01, -5.9640e-01,  8.3145e-01, -1.0032e+00,\n",
       "        -1.2644e-02,  1.9235e+00, -1.7932e+00, -1.0633e+00,  3.2459e+00,\n",
       "        -1.0920e+00, -4.2367e-01,  7.3832e-01, -1.5200e+00, -1.2351e-01,\n",
       "         7.3352e-01, -7.3238e-02,  1.4538e-01, -9.4654e-01,  7.5473e-01,\n",
       "         1.4144e-01,  9.0840e-01, -5.7737e-01, -3.3808e-01, -4.8955e-01,\n",
       "         2.0494e+00, -2.0044e+00,  2.4155e-01,  1.1657e+00,  1.2184e+00,\n",
       "        -1.0330e+00, -7.6570e-01,  5.1279e-01, -1.4796e+00,  2.2514e-02,\n",
       "        -4.5943e-02,  2.3498e-01, -2.7370e-01,  1.5394e+00, -2.1976e+00,\n",
       "         1.8676e+00, -1.4538e+00,  2.6060e+00, -2.1723e+00, -2.8036e-01,\n",
       "         3.2237e-01,  1.0918e+00, -3.7149e-01,  4.4059e-01, -7.9577e-01,\n",
       "        -6.0526e-02,  1.0701e+00, -8.1471e-01, -1.6992e+00,  7.4212e-01,\n",
       "         7.5495e-01,  7.4864e-01, -7.7313e-01, -5.2375e-01,  3.7675e-01,\n",
       "        -4.0850e-01,  1.2004e+00,  4.6865e-01, -1.6073e+00,  6.7742e-01,\n",
       "         7.1563e-01,  1.7280e-02, -7.6481e-01, -4.2100e-01,  8.2012e-02,\n",
       "         1.2153e+00, -3.2478e-01, -4.4597e-01, -2.9895e-01, -2.7709e-01,\n",
       "         4.1768e-01, -1.4928e+00,  5.9025e-01, -3.1162e-01,  1.3604e+00,\n",
       "        -2.3481e-01,  5.4205e-01, -2.1598e+00,  1.7607e+00, -4.2470e-01,\n",
       "        -8.4762e-01,  2.8338e+00, -1.4021e+00,  3.2078e-01,  4.0370e-01,\n",
       "        -1.1758e+00, -7.1561e-01,  3.4458e-01,  5.4432e-01,  2.7551e-01,\n",
       "        -7.7436e-01, -2.4354e-01,  5.6781e-01,  9.7951e-01, -1.1858e-01,\n",
       "        -3.5285e-01,  2.9558e-01, -1.4524e+00,  1.5031e+00,  7.6402e-01,\n",
       "        -1.8190e+00, -4.1544e-01,  9.3048e-01, -5.3767e-01,  1.0129e+00,\n",
       "        -5.5930e-01, -3.6957e-01,  5.3570e-01, -6.6904e-01,  1.4998e+00,\n",
       "        -2.0867e+00,  2.6126e-01,  1.5239e+00, -9.1877e-01, -9.3951e-03,\n",
       "        -1.6563e-01,  6.5176e-01,  3.9702e-01, -6.0368e-01, -6.9102e-01,\n",
       "         8.2188e-01, -1.0287e+00,  1.8628e+00, -3.9520e-01, -1.5262e+00,\n",
       "         2.0444e-01,  4.5228e-01, -4.2325e-01,  3.7887e-01,  1.0060e+00,\n",
       "        -2.6717e-01, -2.6153e-02, -2.9997e-01, -8.3768e-01,  2.2282e+00,\n",
       "        -1.4869e+00,  3.5240e-01,  8.1634e-02, -2.3257e+00,  2.0167e+00,\n",
       "         2.0616e-01, -7.6192e-01,  5.6865e-01,  8.2856e-01, -8.6263e-01,\n",
       "        -4.2126e-01,  1.5935e-01, -6.9180e-01,  2.1090e+00, -2.3102e+00,\n",
       "         8.1642e-01, -3.9487e-01,  1.0551e+00, -1.5393e-01, -2.1057e+00,\n",
       "         1.7053e+00, -2.8349e-02, -3.9315e-01,  1.0132e+00, -2.4827e-01,\n",
       "        -4.5860e-01,  1.9251e+00, -3.0482e+00,  1.1022e+00, -1.0909e+00,\n",
       "         9.5912e-01,  1.2973e+00, -1.6295e-01,  6.3298e-01, -4.0863e-01,\n",
       "        -1.1130e+00, -6.0227e-01, -1.3905e-01,  1.1177e+00, -1.0515e+00,\n",
       "         8.5288e-01, -6.7144e-01,  4.3935e-01,  3.0569e-03, -1.8840e+00,\n",
       "         3.9655e-01,  2.2855e+00, -1.3982e+00, -2.0131e-01,  1.8237e+00,\n",
       "        -1.4273e+00,  8.8549e-01, -1.5591e+00,  1.4639e+00, -1.6776e-01,\n",
       "        -1.7641e-01, -5.7138e-01,  6.4976e-01,  8.1290e-01, -2.6225e-01,\n",
       "         3.7479e-01, -2.4211e-01,  2.3877e-02,  7.7638e-01, -1.7467e+00,\n",
       "         3.5175e-01, -1.1778e-02,  3.7816e-01, -4.0770e-01, -8.0850e-01,\n",
       "         3.1500e-01,  1.6938e-01, -1.1470e+00,  2.0665e+00,  4.5281e-01,\n",
       "        -1.6891e+00, -5.1289e-01,  4.4047e-01,  3.0270e-01,  1.3180e+00,\n",
       "        -8.1328e-01,  6.2050e-01, -3.9058e-01, -1.0455e+00, -8.4157e-01,\n",
       "         2.1754e+00, -9.9886e-01, -4.9515e-01,  1.0083e+00,  3.4330e-01,\n",
       "        -6.4663e-01,  1.0002e+00,  1.1653e+00, -1.7406e+00, -5.4612e-01,\n",
       "         9.1515e-01, -1.6087e+00,  8.7007e-01, -8.0687e-01, -7.5317e-01,\n",
       "         1.4396e+00,  6.4315e-01,  6.4863e-02, -1.9520e+00,  9.4629e-01,\n",
       "         4.2130e-01, -2.1827e-01, -9.8392e-01])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_space_paralell[0,0,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.5887e-01,  1.5192e+00, -2.3931e-01, -4.9895e-01, -1.2672e+00,\n",
       "         1.2690e+00,  5.5180e-01, -7.0137e-01, -4.0276e-01,  1.1063e+00,\n",
       "        -1.7431e+00,  2.2606e-01,  1.8534e-02,  1.1908e+00,  3.4091e-01,\n",
       "        -1.1821e+00, -7.8780e-01,  3.5548e-01,  1.0009e+00,  1.0632e-01,\n",
       "        -3.2831e-01,  6.6244e-01, -1.4574e-01, -1.0425e+00, -5.1515e-01,\n",
       "         2.0787e+00,  6.6962e-02, -1.4101e+00,  3.5819e-01, -2.0816e-01,\n",
       "        -1.3312e+00, -4.5513e-01,  4.4870e-01,  4.9707e-01,  2.5160e-01,\n",
       "         1.5117e+00, -2.7640e+00,  1.7142e+00,  3.4535e-01, -9.5751e-01,\n",
       "         2.1636e+00, -1.3937e+00,  3.3830e-01, -1.8516e-01,  2.8886e-01,\n",
       "         2.2719e-01, -6.0110e-01, -3.7749e-02,  7.9825e-01, -2.2316e-01,\n",
       "         5.4351e-01, -2.3285e+00,  1.2120e+00,  1.3499e+00, -1.7468e+00,\n",
       "        -1.8926e-01, -4.4488e-01,  1.6370e+00, -1.1455e+00,  5.6134e-02,\n",
       "         6.1739e-01,  2.5897e-01, -1.1238e-02, -2.6212e-01,  3.1307e-02,\n",
       "        -3.6046e-01,  1.9612e+00, -9.9431e-01, -1.8707e+00,  3.3488e-01,\n",
       "         3.7976e-01, -3.2490e-01,  5.6010e-01, -5.0206e-01,  1.9878e-01,\n",
       "         1.5906e+00, -9.2202e-01, -5.4628e-01, -3.8638e-01,  6.7701e-01,\n",
       "        -1.8551e+00,  8.1489e-01,  1.5125e+00, -2.0028e-01, -7.0447e-02,\n",
       "        -1.5750e-01,  2.5795e+00, -2.2891e+00, -1.0721e+00,  6.8359e-01,\n",
       "        -1.0705e+00,  8.0446e-01, -5.0054e-01,  1.9562e+00, -3.0376e+00,\n",
       "         1.0187e+00,  1.5471e-01,  6.2300e-02,  1.0780e+00, -1.3124e+00,\n",
       "         1.0291e+00, -1.2619e-01,  2.3308e-01,  4.6691e-01, -9.7600e-01,\n",
       "         8.1542e-01, -9.3589e-01,  6.1739e-01, -2.1243e-02, -1.3588e+00,\n",
       "         1.1577e+00, -3.1527e-02, -1.1352e+00,  3.9337e-01,  2.0088e-01,\n",
       "         1.7510e+00, -1.8462e+00,  9.0798e-01, -7.4378e-01,  4.0243e-01,\n",
       "         7.0501e-02, -7.5712e-01,  1.3882e+00, -1.2942e+00, -2.1508e-01,\n",
       "         1.3998e+00, -1.5090e+00, -4.2789e-01,  1.0214e+00,  3.1513e-01,\n",
       "        -1.0907e+00,  7.8192e-01,  1.7878e+00, -1.8263e+00,  2.4576e-01,\n",
       "         1.4453e+00, -2.2494e-01, -8.0719e-01,  6.9224e-01, -1.4605e-01,\n",
       "        -2.2172e-01, -4.7186e-01,  1.9929e-01, -5.6445e-01,  4.6437e-02,\n",
       "        -5.7804e-01,  7.0168e-01, -2.9422e-01, -6.5954e-01,  1.3863e+00,\n",
       "        -8.0903e-01, -9.0469e-01,  1.3607e+00,  5.1854e-01, -3.2137e-01,\n",
       "        -1.2875e+00,  1.5410e+00, -5.7165e-01,  1.2456e-01,  1.3840e+00,\n",
       "        -1.9995e+00,  3.5822e-01,  1.0151e+00, -1.8127e-01, -4.7247e-01,\n",
       "        -1.1546e+00,  6.5900e-01,  1.0720e+00, -6.0584e-01,  1.2438e+00,\n",
       "        -2.1915e+00,  1.2547e+00, -2.4979e-01, -4.7562e-02,  6.0863e-01,\n",
       "        -3.7850e-01,  2.4582e-01, -1.8402e-01, -4.0823e-02, -1.2630e+00,\n",
       "         1.7934e+00, -4.5852e-01, -6.1057e-01,  7.6922e-01,  1.0000e-01,\n",
       "        -8.7129e-01,  1.2575e-01, -1.6938e-02,  8.2695e-02,  4.7032e-01,\n",
       "        -1.0233e+00, -9.6425e-01,  1.4951e+00, -1.6616e+00,  1.0369e+00,\n",
       "         1.5422e+00, -3.6988e-01, -1.8290e+00,  1.2169e+00, -1.6221e+00,\n",
       "         1.0629e+00,  5.7622e-01,  1.0808e+00, -1.0598e+00, -3.7464e-01,\n",
       "        -8.0165e-01,  3.6734e-01,  1.0252e+00,  6.2546e-02, -1.4832e+00,\n",
       "         1.4258e+00,  6.2759e-01, -2.0650e+00,  2.3637e+00, -6.2475e-01,\n",
       "         7.2607e-02,  1.0529e-01, -1.0836e+00, -8.1577e-01, -4.0202e-01,\n",
       "         1.6438e+00, -5.4273e-01,  4.8675e-01,  1.0961e+00, -1.3784e+00,\n",
       "         9.2545e-01,  9.4935e-01, -2.2071e+00, -3.7920e-01,  1.8237e+00,\n",
       "         5.7227e-02, -9.1851e-01, -1.1431e+00,  5.8274e-01,  1.7736e-01,\n",
       "         4.6752e-01, -2.5604e-01,  1.3582e+00, -4.5342e-01,  3.0559e-01,\n",
       "        -7.5229e-01, -1.2753e+00,  1.5970e+00,  5.4402e-01, -8.4945e-01,\n",
       "        -6.2085e-01,  1.5971e-01,  9.3169e-01, -3.7399e-02,  2.2801e-01,\n",
       "        -1.6181e+00,  8.0465e-01, -5.9640e-01,  8.3145e-01, -1.0032e+00,\n",
       "        -1.2644e-02,  1.9235e+00, -1.7932e+00, -1.0633e+00,  3.2459e+00,\n",
       "        -1.0920e+00, -4.2367e-01,  7.3832e-01, -1.5200e+00, -1.2351e-01,\n",
       "         7.3352e-01, -7.3238e-02,  1.4538e-01, -9.4654e-01,  7.5473e-01,\n",
       "         1.4143e-01,  9.0840e-01, -5.7737e-01, -3.3808e-01, -4.8955e-01,\n",
       "         2.0494e+00, -2.0044e+00,  2.4155e-01,  1.1657e+00,  1.2184e+00,\n",
       "        -1.0330e+00, -7.6570e-01,  5.1279e-01, -1.4796e+00,  2.2514e-02,\n",
       "        -4.5943e-02,  2.3498e-01, -2.7370e-01,  1.5394e+00, -2.1976e+00,\n",
       "         1.8676e+00, -1.4538e+00,  2.6060e+00, -2.1723e+00, -2.8036e-01,\n",
       "         3.2237e-01,  1.0918e+00, -3.7149e-01,  4.4059e-01, -7.9577e-01,\n",
       "        -6.0526e-02,  1.0701e+00, -8.1471e-01, -1.6992e+00,  7.4212e-01,\n",
       "         7.5495e-01,  7.4864e-01, -7.7313e-01, -5.2375e-01,  3.7675e-01,\n",
       "        -4.0850e-01,  1.2004e+00,  4.6865e-01, -1.6073e+00,  6.7742e-01,\n",
       "         7.1563e-01,  1.7280e-02, -7.6481e-01, -4.2100e-01,  8.2012e-02,\n",
       "         1.2153e+00, -3.2478e-01, -4.4597e-01, -2.9895e-01, -2.7709e-01,\n",
       "         4.1768e-01, -1.4928e+00,  5.9025e-01, -3.1162e-01,  1.3604e+00,\n",
       "        -2.3481e-01,  5.4205e-01, -2.1598e+00,  1.7607e+00, -4.2470e-01,\n",
       "        -8.4762e-01,  2.8338e+00, -1.4021e+00,  3.2078e-01,  4.0370e-01,\n",
       "        -1.1758e+00, -7.1561e-01,  3.4458e-01,  5.4432e-01,  2.7551e-01,\n",
       "        -7.7436e-01, -2.4354e-01,  5.6781e-01,  9.7951e-01, -1.1858e-01,\n",
       "        -3.5285e-01,  2.9558e-01, -1.4524e+00,  1.5031e+00,  7.6402e-01,\n",
       "        -1.8190e+00, -4.1544e-01,  9.3048e-01, -5.3767e-01,  1.0129e+00,\n",
       "        -5.5930e-01, -3.6957e-01,  5.3570e-01, -6.6904e-01,  1.4998e+00,\n",
       "        -2.0867e+00,  2.6126e-01,  1.5239e+00, -9.1877e-01, -9.3950e-03,\n",
       "        -1.6563e-01,  6.5176e-01,  3.9702e-01, -6.0368e-01, -6.9102e-01,\n",
       "         8.2188e-01, -1.0287e+00,  1.8628e+00, -3.9520e-01, -1.5262e+00,\n",
       "         2.0444e-01,  4.5228e-01, -4.2325e-01,  3.7887e-01,  1.0060e+00,\n",
       "        -2.6717e-01, -2.6153e-02, -2.9997e-01, -8.3768e-01,  2.2282e+00,\n",
       "        -1.4869e+00,  3.5240e-01,  8.1634e-02, -2.3257e+00,  2.0167e+00,\n",
       "         2.0616e-01, -7.6192e-01,  5.6865e-01,  8.2856e-01, -8.6262e-01,\n",
       "        -4.2126e-01,  1.5935e-01, -6.9180e-01,  2.1090e+00, -2.3102e+00,\n",
       "         8.1642e-01, -3.9487e-01,  1.0551e+00, -1.5393e-01, -2.1057e+00,\n",
       "         1.7053e+00, -2.8349e-02, -3.9315e-01,  1.0132e+00, -2.4827e-01,\n",
       "        -4.5860e-01,  1.9251e+00, -3.0482e+00,  1.1022e+00, -1.0909e+00,\n",
       "         9.5912e-01,  1.2973e+00, -1.6295e-01,  6.3298e-01, -4.0863e-01,\n",
       "        -1.1130e+00, -6.0227e-01, -1.3905e-01,  1.1177e+00, -1.0515e+00,\n",
       "         8.5288e-01, -6.7144e-01,  4.3935e-01,  3.0571e-03, -1.8840e+00,\n",
       "         3.9655e-01,  2.2855e+00, -1.3982e+00, -2.0131e-01,  1.8237e+00,\n",
       "        -1.4273e+00,  8.8549e-01, -1.5591e+00,  1.4639e+00, -1.6776e-01,\n",
       "        -1.7641e-01, -5.7138e-01,  6.4976e-01,  8.1290e-01, -2.6225e-01,\n",
       "         3.7479e-01, -2.4211e-01,  2.3877e-02,  7.7638e-01, -1.7467e+00,\n",
       "         3.5175e-01, -1.1779e-02,  3.7816e-01, -4.0770e-01, -8.0850e-01,\n",
       "         3.1500e-01,  1.6938e-01, -1.1470e+00,  2.0665e+00,  4.5281e-01,\n",
       "        -1.6891e+00, -5.1289e-01,  4.4047e-01,  3.0270e-01,  1.3180e+00,\n",
       "        -8.1328e-01,  6.2050e-01, -3.9058e-01, -1.0455e+00, -8.4157e-01,\n",
       "         2.1754e+00, -9.9886e-01, -4.9515e-01,  1.0083e+00,  3.4330e-01,\n",
       "        -6.4663e-01,  1.0002e+00,  1.1653e+00, -1.7406e+00, -5.4612e-01,\n",
       "         9.1515e-01, -1.6087e+00,  8.7007e-01, -8.0687e-01, -7.5317e-01,\n",
       "         1.4396e+00,  6.4315e-01,  6.4863e-02, -1.9520e+00,  9.4629e-01,\n",
       "         4.2130e-01, -2.1827e-01, -9.8392e-01])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_space[0,0,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0523,  0.0668, -1.1534,  ...,  0.0197,  0.9567, -1.1148],\n",
       "          [-0.5544, -0.2033,  2.9779,  ..., -1.5160, -2.0545,  4.1092],\n",
       "          [-0.3861, -0.7471,  1.3966,  ...,  1.7326, -1.7076,  0.4324],\n",
       "          ...,\n",
       "          [ 0.1682,  0.1178,  0.3977,  ...,  0.3496, -0.3953,  0.1881],\n",
       "          [ 0.1698,  0.1175,  0.4022,  ...,  0.3480, -0.3946,  0.1885],\n",
       "          [ 0.1715,  0.1171,  0.4068,  ...,  0.3464, -0.3939,  0.1888]],\n",
       "\n",
       "         [[-0.7677,  1.7796, -1.5688,  ..., -0.9761,  1.9610, -2.5358],\n",
       "          [ 3.4110, -4.5800,  1.1505,  ...,  0.4845, -1.0007,  2.9399],\n",
       "          [ 1.1078, -1.1725,  0.4343,  ...,  1.4548, -1.1677,  1.0524],\n",
       "          ...,\n",
       "          [ 0.7071, -0.3788, -0.1714,  ..., -0.4819,  0.1764,  0.2281],\n",
       "          [ 0.7112, -0.3798, -0.1743,  ..., -0.4875,  0.1796,  0.2307],\n",
       "          [ 0.7152, -0.3806, -0.1771,  ..., -0.4931,  0.1829,  0.2332]]]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_space_paralell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 1, 100])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Z_expanded @ U_expanded).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 100, 1])\n",
      "torch.Size([1, 2, 200])\n",
      "torch.Size([1, 1, 100, 1])\n"
     ]
    }
   ],
   "source": [
    "tmp = (Z_expanded @ U_expanded).permute(0,1,3,2)\n",
    "print(tmp.shape)\n",
    "\n",
    "# take first len(lam) of V\n",
    "V_lam = V_expanded[:,:,:len(lam),:] \n",
    "term = (fi * (tmp * V_lam) / lam.reshape(1, 1, -1, 1)).sum(dim=2)\n",
    "tau_space_paralell[:, :, :, h] = term\n",
    "\n",
    "print(term.shape)\n",
    "print(fi.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 200, 498])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_space_paralell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 200])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 100])\n",
      "torch.Size([200, 200])\n",
      "torch.Size([1, 2, 100, 498])\n",
      "##############################\n",
      "torch.Size([100])\n",
      "torch.Size([1, 1, 100, 100])\n",
      "torch.Size([1, 2, 1, 100])\n",
      "torch.Size([1, 1, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "print(U.shape)\n",
    "print(V.shape)\n",
    "print(Z.shape)\n",
    "print('##############################')\n",
    "\n",
    "print(fi.shape)\n",
    "print(U_expanded.shape)\n",
    "print(Z_expanded.shape)\n",
    "print(V_expanded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [2, 100] but got: [2, 200].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mU_expanded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mZ_expanded\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mV_expanded\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [2, 100] but got: [2, 200]."
     ]
    }
   ],
   "source": [
    "U_expanded * Z_expanded @ V_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(K)\n",
    "print(len(lam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200,)\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(new.shape)\n",
    "print(len(lam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#smooth tau-space (it might not be necessary, use for a smoother visualization):\n",
    "# for h in range(0,num_h-2):\n",
    "#     tau_space[:,h]=savgol_filter(tau_space[:,h], 11, 1)\n",
    "\n",
    "#Normalization (it is not really necessary for this very short temporal horizon T=4):\n",
    "tau_space[tau_space<0]=0 #make all probabilities positive\n",
    "# do for several batches and actions -> parallelize TODO\n",
    "for batch in range(batch_size):\n",
    "    for a in range(num_actions):\n",
    "        for i in range(0,K):\n",
    "            if torch.nansum(tau_space[batch,a,i,:])>0.0:\n",
    "                tau_space[batch,a,i,:]=tau_space[batch,a,i,:]/torch.nansum(tau_space[batch,a,i,:])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(tau_space_paralell, tau_space, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 200, 498])\n"
     ]
    }
   ],
   "source": [
    "print(tau_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_space_paralell_tmp = tau_space_paralell.clone()\n",
    "\n",
    "\n",
    "tau_space_paralell_tmp[tau_space_paralell_tmp < 0] = 0\n",
    "\n",
    "sum_tau = torch.nansum(tau_space_paralell_tmp, dim=-1, keepdim=True)\n",
    "mask = sum_tau > 0\n",
    "sum_tau[sum_tau == 0] = 1\n",
    "\n",
    "tau_space_paralell_tmp = tau_space_paralell_tmp / sum_tau\n",
    "\n",
    "mask_expanded = mask.expand_as(tau_space_paralell_tmp)\n",
    "tau_space_paralell_tmp[~mask_expanded] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(tau_space_paralell_tmp, tau_space, atol=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 200, 1])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 200, 498])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_space_paralell_tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (199200) must match the existing size (400) at non-singleton dimension 0.  Target sizes: [199200].  Tensor sizes: [400]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[209], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m mask \u001b[38;5;241m=\u001b[39m sum_tau \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m mask_expanded \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mexpand_as(tau_space_paralell_tmp)\n\u001b[0;32m----> 8\u001b[0m tau_space_paralell_tmp[mask_expanded] \u001b[38;5;241m=\u001b[39m tau_space_paralell_tmp[mask_expanded] \u001b[38;5;241m/\u001b[39m \u001b[43msum_tau\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtau_space_paralell_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_expanded\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (199200) must match the existing size (400) at non-singleton dimension 0.  Target sizes: [199200].  Tensor sizes: [400]"
     ]
    }
   ],
   "source": [
    "tau_space_paralell_tmp = tau_space_paralell.clone()\n",
    "\n",
    "tau_space_paralell_tmp[tau_space_paralell_tmp<0] = 0\n",
    "sum_tau = torch.nansum(tau_space_paralell_tmp, dim=-1, keepdim=True)\n",
    "mask = sum_tau > 0\n",
    "mask_expanded = mask.expand_as(tau_space_paralell_tmp)\n",
    "\n",
    "tau_space_paralell_tmp[mask_expanded] = tau_space_paralell_tmp[mask_expanded] / sum_tau[mask].expand_as(tau_space_paralell_tmp[mask_expanded])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([199200])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (199200) must match the existing size (400) at non-singleton dimension 0.  Target sizes: [199200].  Tensor sizes: [400]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[212], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(tau_space_paralell_tmp[mask_expanded]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msum_tau\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_as\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtau_space_paralell_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask_expanded\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (199200) must match the existing size (400) at non-singleton dimension 0.  Target sizes: [199200].  Tensor sizes: [400]"
     ]
    }
   ],
   "source": [
    "print(tau_space_paralell_tmp[mask_expanded].shape)\n",
    "print(sum_tau[mask].expand_as(tau_space_paralell_tmp[mask_expanded]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [1, 2, 200, 1] at index 3 does not match the shape of the indexed tensor [1, 2, 200, 498] at index 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[207], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(sum_tau[mask]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtau_space_paralell_tmp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [1, 2, 200, 1] at index 3 does not match the shape of the indexed tensor [1, 2, 200, 498] at index 3"
     ]
    }
   ],
   "source": [
    "print(sum_tau[mask].shape)\n",
    "print(tau_space_paralell_tmp[mask].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [1, 2, 200, 1] at index 3 does not match the shape of the indexed tensor [1, 2, 200, 498] at index 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[203], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtau_space_paralell\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpositv_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [1, 2, 200, 1] at index 3 does not match the shape of the indexed tensor [1, 2, 200, 498] at index 3"
     ]
    }
   ],
   "source": [
    "tau_space_paralell[positv_mask.unsqueeze(-1)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 200, 498])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_space_paralell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 200])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nansum(tau_space_paralell, dim=3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 200])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positv_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 200, 1])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positv_mask.unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distrl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
