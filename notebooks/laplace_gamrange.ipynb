{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.replay_memory import ReplayMemory, Transition\n",
    "from utils.reproducibility import set_seed\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from Networks.LaplaceDQN_net import * \n",
    "from Networks.LaplaceDQN_net_monot_multgam import *\n",
    "from utils.Inverse_Laplace import SVD_approximation_inverse_Laplace\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import gym\n",
    "from config import Config\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetMultgam(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.input_dim = config.input_dim + 1 # add one for the discount factor \n",
    "        self.action_dim = config.action_dim\n",
    "        self.num_sensitivities = config.num_sensitivities\n",
    "        output_dim = self.action_dim * self.num_sensitivities\n",
    "\n",
    "        super(DQNNetMultgam, self).__init__()\n",
    "        self.layer1 = nn.Linear(self.input_dim, 64) # originally 128\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, output_dim)\n",
    "        self.reshape_layer = lambda x: x.view(-1, self.action_dim, self.num_sensitivities)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x)) \n",
    "        x = self.reshape_layer(x)\n",
    "        \n",
    "        x = torch.cumsum(x, dim=2)\n",
    "        x = torch.flip(x, dims=[2])\n",
    "                \n",
    "        diffs = torch.diff(x, dim=2)\n",
    "        #print(x.shape, diffs.shape, x, diffs)\n",
    "        # assert x == diffs\n",
    "        assert torch.all(diffs <= 0), \"Output Tensor is not non-increasing along dimension 2\"\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNNetMultgamInv(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.input_dim = config.input_dim + 1 # add one for the discount factor \n",
    "        self.action_dim = config.action_dim\n",
    "        self.num_sensitivities = config.num_sensitivities\n",
    "        output_dim = self.action_dim * self.num_sensitivities\n",
    "\n",
    "        super(DQNNetMultgamInv, self).__init__()\n",
    "        self.low_net = DQNNetMultgam(config)\n",
    "        self.med_net = DQNNetMultgam(config)\n",
    "        self.high_net = DQNNetMultgam(config)\n",
    "\n",
    "        self.low_range = 0.5\n",
    "        self.med_range = 0.75\n",
    "        self.high_range = 1.0\n",
    "        \n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        # x: [B, num_states + 1(gamma-dim)]\n",
    "        gamm_tens = x[:,-1]\n",
    "        low_gam_mask = (gamm_tens >= 0.0) & (gamm_tens < 0.5)\n",
    "        med_gam_mask = (gamm_tens >= 0.5) &  (gamm_tens < 0.75)\n",
    "        high_gam_mask = (gamm_tens >= 0.75) &  (gamm_tens <= 1)\n",
    "\n",
    "        # assumed gammas in range [0,1]\n",
    "        assert (low_gam_mask.sum()+med_gam_mask.sum()+high_gam_mask.sum()) == gamm_tens.shape[0]\n",
    "\n",
    "        # Collect the outputs from each network\n",
    "        low_out = self.low_net(x[low_gam_mask])\n",
    "        med_out = self.med_net(x[med_gam_mask])\n",
    "        high_out = self.high_net(x[high_gam_mask])\n",
    "\n",
    "        # Initialize the result tensor with the same dtype and device as input x\n",
    "        res_tens = torch.zeros(x.shape[0], self.action_dim, self.num_sensitivities, dtype=x.dtype, device=x.device)\n",
    "\n",
    "        # Indexes for placing outputs in the result tensor\n",
    "        low_indexes = low_gam_mask.nonzero(as_tuple=True)[0]\n",
    "        med_indexes = med_gam_mask.nonzero(as_tuple=True)[0]\n",
    "        high_indexes = high_gam_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Place the outputs in the correct positions\n",
    "        res_tens[low_indexes] = low_out\n",
    "        res_tens[med_indexes] = med_out\n",
    "        res_tens[high_indexes] = high_out\n",
    "        \n",
    "        return self.low_net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = 'CartPole-v1'\n",
    "env = gym.make(game, None) \n",
    "config = Config(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceDQNAgentMultgamIntv:\n",
    "\n",
    "    def __init__(self, game, config):\n",
    "        # env = gym.make(game, None) \n",
    "        # config = Config(env)\n",
    "        self.config = config\n",
    "\n",
    "        self.envs = [gym.make(game, None) for _ in range(config.num_gamma)]\n",
    "        # self.envs = [env] + [gym.make(game, None) for _ in range(config.num_gamma-1)]\n",
    "\n",
    "        for env in self.envs:\n",
    "            env.action_space.seed(config.seed)\n",
    "        \n",
    "        self.config = config \n",
    "        self.input_dim = config.input_dim\n",
    "        self.action_dim = config.action_dim\n",
    "\n",
    "        self.num_episodes = config.num_episodes\n",
    "        self.initial_number_steps = config.initial_number_steps\n",
    "        self.later_number_steps = config.later_number_steps\n",
    "        self.max_steps = self.initial_number_steps\n",
    "        self.BATCH_SIZE = config.BATCH_SIZE\n",
    "        self.LR = config.LR\n",
    "        self.TAU = config.TAU\n",
    "        self.device = config.device\n",
    "        \n",
    "        self.num_gamma = config.num_gamma \n",
    "        self.gamma_min = config.gamma_min\n",
    "        self.gamma_max = config.gamma_max\n",
    "        start = 1 / np.log(self.gamma_min) \n",
    "        #start = 1 / np.log(0.99) # with 1 gamma TODO\n",
    "        end = 1 / np.log(self.gamma_max)\n",
    "        #end = 1 / np.log(0.95)   \n",
    "        self.gammas = torch.exp(torch.true_divide(1, torch.linspace(start, end, self.num_gamma)))\n",
    "        self.num_sensitivities = config.num_sensitivities \n",
    "        self.rmin = config.rmin\n",
    "        self.rmax = config.rmax\n",
    "        assert self.rmin < self.rmax, \"The reward range is not valid\"\n",
    "        self.sensitivity_step = (self.rmax - self.rmin) / self.num_sensitivities\n",
    "        self.sensitivities = torch.arange(self.rmin, self.rmax, self.sensitivity_step)\n",
    "        self.middle_sensitivities = torch.tensor([torch.true_divide(self.sensitivities[i] + self.sensitivities[i+1], 2) for i in range(self.sensitivities.shape[0]-1)])\n",
    "        self.activ_sharpness = config.activ_sharpness\n",
    "        \n",
    "        self.num_gamma_to_tau = config.num_gamma_to_tau\n",
    "        self.gamma_to_tau_min = config.gamma_to_tau_min\n",
    "        self.gamma_to_tau_max = config.gamma_to_tau_max\n",
    "        start = 1 / np.log(self.gamma_to_tau_min) \n",
    "        end = 1 / np.log(self.gamma_to_tau_max)   \n",
    "        self.gammas_to_tau = torch.exp(torch.true_divide(1, torch.linspace(start, end, self.num_gamma_to_tau)))\n",
    "        self.K = config.K\n",
    "        self.time_horizon_change = config.time_horizon_change\n",
    "        \n",
    "        # taus=torch.linspace(0.01,3,self.num_gamma) # as in implementation from Pablo Tano\n",
    "        # self.gammas=torch.exp(-1/taus)\n",
    "        print(\"Gammas : \", self.gammas)\n",
    "        \n",
    "        self.total_steps = [0 for _ in range(self.num_gamma)]\n",
    "\n",
    "        # reproducibility\n",
    "        self.seed = config.seed\n",
    "        set_seed(self.seed)\n",
    "        \n",
    "        self.dir = os.getcwd()\n",
    "\n",
    "        # copying weights of base_net to policy_net and target_net\n",
    "        self.policy_net = DQNNetMultgamInv(self.config) \n",
    "        self.target_net = DQNNetMultgamInv(self.config)\n",
    "        #self.policy_net = DQNNetMultgam(self.config)\n",
    "        #self.target_net = DQNNetMultgam(self.config)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr=self.LR, amsgrad=True)\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        self.replay_buffer_size = config.replay_buffer_size\n",
    "        self.replay_memory = [ReplayMemory(self.replay_buffer_size) for _ in range(self.num_gamma)] \n",
    "        \n",
    "        self.episode_durations = []\n",
    "        self.check_model_improved = [torch.tensor([0]) for _ in range(self.num_gamma)]\n",
    "        self.best_max = [torch.tensor([0]) for _ in range(self.num_gamma)]\n",
    "\n",
    "        # for select action (epsilon-greedy)\n",
    "        self.steps_done = [0 for _ in range(self.num_gamma)]\n",
    "        \n",
    "        # save for plotting evolution during training for each gamma\n",
    "        self.model_reward_hist = [[] for _ in range(self.num_gamma)]\n",
    "        self.model_loss_hist = [[] for _ in range(self.num_gamma)]\n",
    "    \n",
    "    \n",
    "    def transition(self):\n",
    "        \"\"\"\n",
    "        In transition, the agent simply plays and records\n",
    "        [current_state, action, reward, next_state, done]\n",
    "        in the replay_memory\n",
    "\n",
    "        Updating the weights of the neural network happens\n",
    "        every single time the replay buffer size is reached.\n",
    "\n",
    "        done: boolean, whether the game has ended or not.\n",
    "        \"\"\"\n",
    "        # Start time\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "        # multiple environments for every gamma\n",
    "        self.states = [torch.from_numpy(env.reset(seed=self.seed)[0]).float().unsqueeze(0) for env in self.envs] \n",
    "            \n",
    "        for i_episode in range(self.num_episodes):\n",
    "            # No horizon change during training for now\n",
    "            # if self.time_horizon_change is not None and i_episode == self.time_horizon_change:\n",
    "            #     self.max_steps = self.later_number_steps\n",
    "\n",
    "            for gamma_idx, gamma in enumerate(self.gammas):\n",
    "                print('Episode: {} Reward: {} Max_Reward: {}'.format(i_episode, self.check_model_improved[gamma_idx].item(), self.best_max[gamma_idx].item()))\n",
    "                print('-' * 64)\n",
    "                self.check_model_improved[gamma_idx] = 0\n",
    "                for t in count():    \n",
    "                    env = self.envs[gamma_idx]\n",
    "                    state = self.states[gamma_idx]\n",
    "                    \n",
    "                    action = self.select_action(gamma_idx)\n",
    "                    # No horizon change during training for now\n",
    "                    # if (self.time_horizon_change is None) or (self.time_horizon_change is not None and i_episode < self.time_horizon_change):\n",
    "                    #     action = self.select_action(gamma_idx)\n",
    "                    # else:\n",
    "                    #     action = self.select_action_after_H_change(gamma_idx) \n",
    "                        \n",
    "                    observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "                    reward = torch.tensor([reward], device=self.device)\n",
    "                    done = terminated or truncated\n",
    "\n",
    "                    # if terminated or t == self.max_steps:\n",
    "                    if terminated:\n",
    "                        next_state = None\n",
    "                    else:\n",
    "                        next_state = torch.tensor(observation, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "                    # Store the transition in memory\n",
    "                    self.replay_memory[gamma_idx].push(state, action, next_state, reward)\n",
    "\n",
    "                    # Move to the next state\n",
    "                    self.states[gamma_idx] = next_state\n",
    "                    self.total_steps[gamma_idx] += 1\n",
    "\n",
    "                    # Perform one step of the optimization (on the policy network)\n",
    "                    self.train_by_replay(gamma_idx) \n",
    "\n",
    "                    # Soft update of the target network's weights \n",
    "                    # θ′ ← τ θ + (1 −τ )θ′\n",
    "                    # previous implementation updates were done for any episode where the reward is higher\n",
    "                    target_net_state_dict = self.target_net.state_dict()\n",
    "                    policy_net_state_dict = self.policy_net.state_dict()\n",
    "                    for key in policy_net_state_dict:\n",
    "                        target_net_state_dict[key] = policy_net_state_dict[key]*self.TAU + target_net_state_dict[key]*(1-self.TAU)\n",
    "                    self.target_net.load_state_dict(target_net_state_dict)\n",
    "                    \n",
    "                    self.check_model_improved[gamma_idx] += reward \n",
    "                    # if done or t == self.max_steps:\n",
    "                    if done:\n",
    "                        self.episode_durations.append(t + 1)\n",
    "                        self.model_reward_hist[gamma_idx].append(self.check_model_improved[gamma_idx].detach().numpy())\n",
    "                        self.states[gamma_idx] = torch.from_numpy(self.envs[gamma_idx].reset(seed=self.seed)[0]).float().unsqueeze(0)\n",
    "                        break \n",
    "\n",
    "                if self.check_model_improved[gamma_idx] > self.best_max[gamma_idx]:\n",
    "                    self.best_max[gamma_idx] = self.check_model_improved[gamma_idx]\n",
    "                    \n",
    "        folder = 'Results'\n",
    "        subfolder_reward = 'Rewards'\n",
    "        subfolder_loss = \"Losses\"\n",
    "        subfolder_weights = \"Weights\"\n",
    "        filename_rewards = 'rewards_laplace_mult.npy'\n",
    "        filename_losses = 'losses_laplace_mult.npy'\n",
    "        filename_weights = 'policy_net_weights_laplace_mult.pth'\n",
    "        path_rewards = os.path.join(self.dir, folder, subfolder_reward, filename_rewards)\n",
    "        path_losses = os.path.join(self.dir, folder, subfolder_loss, filename_losses)\n",
    "        path_weights = os.path.join(self.dir, folder, subfolder_weights, filename_weights)\n",
    "                \n",
    "        # Save the losses and rewards to numpy arrays\n",
    "        cum_reward_per_episode = np.array([self.model_reward_hist[i] for i in range(self.num_gamma)])\n",
    "        np.save(path_rewards, cum_reward_per_episode)\n",
    "        \n",
    "        # arrange dimension difference due to difference transition lengths\n",
    "        # append -1, to concatenate them, since loss cannot be -1\n",
    "        vectors = self.model_loss_hist\n",
    "        max_length = max(len(vec) for vec in vectors)\n",
    "        padded_vectors = [np.pad(vec, (0, max_length - len(vec)), 'constant', constant_values=-1) for vec in vectors]\n",
    "        loss_result = np.vstack(padded_vectors)\n",
    "        np.save(path_losses, np.array(loss_result)) # check if I can save a numpy array of lists\n",
    "\n",
    "        torch.save(self.policy_net.state_dict(), path_weights)\n",
    "        \n",
    "        # End time\n",
    "        self.end_time = time.time()\n",
    "        \n",
    "        \n",
    "    def train_by_replay(self, gamma_idx):\n",
    "        \"\"\"\n",
    "        TD update by replaying the history.\n",
    "        \"\"\"\n",
    "        # step 1: generate replay samples (size = self.batch_size) from the replay buffer\n",
    "        # e.g. uniform random replay or prioritize experience replay\n",
    "        if len(self.replay_memory[gamma_idx]) < self.BATCH_SIZE:\n",
    "            return\n",
    "        transitions = self.replay_memory[gamma_idx].sample(self.BATCH_SIZE)\n",
    "\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        # (a final state would've been the one after which simulation ended)\n",
    "        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=self.device, dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                    if s is not None])\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        \n",
    "        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "        # columns of actions taken. These are the actions which would've been taken\n",
    "        # for each batch state according to policy_net\n",
    "        input = torch.cat((state_batch, torch.tensor([self.gammas[gamma_idx]]).unsqueeze(0).repeat(state_batch.shape[0], 1)), dim=1)  \n",
    "        assert state_batch.shape[0] == input.shape[0]\n",
    "        output = self.policy_net(input) \n",
    "        state_action_values = output[torch.arange(output.size(0)), action_batch.squeeze()]\n",
    "        \n",
    "        # Compute V(s_{t+1}) for all next states.\n",
    "        # Expected values of actions for non_final_next_states are computed based\n",
    "        # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "        # This is merged based on the mask, such that we'll have either the expected\n",
    "        # state value or 0 in case the state was final.\n",
    "        next_output = torch.zeros(self.BATCH_SIZE, self.action_dim, self.num_sensitivities, device=self.device) # for 1 sensitivity \n",
    "        with torch.no_grad():\n",
    "            next_inputs = torch.cat((non_final_next_states, torch.tensor([self.gammas[gamma_idx]]).unsqueeze(0).repeat(non_final_next_states.shape[0], 1)), dim=1) \n",
    "            next_output[non_final_mask] = self.target_net(next_inputs)\n",
    "            action_values = torch.sum((next_output[:, :, :-1] - next_output[:, :, 1:]) * self.middle_sensitivities, dim=2)\n",
    "            max_action_values = action_values.max(1).indices\n",
    "            next_state_values = next_output[torch.arange(next_output.size(0)), max_action_values]        \n",
    "        \n",
    "        rewards_thresh = torch.nn.functional.sigmoid(self.activ_sharpness*(reward_batch.unsqueeze(-1).repeat(1, self.sensitivities.shape[0])-self.sensitivities.unsqueeze(0).repeat(reward_batch.shape[0], 1)))        \n",
    "        \n",
    "        assert torch.all((rewards_thresh <= 1) & (rewards_thresh >= 0)), \"Rewards after activation should be between 0 or 1\"\n",
    "        \n",
    "        # Compute the expected Q values \n",
    "        expected_state_action_values = (next_state_values * self.gammas[gamma_idx]) + rewards_thresh \n",
    "        \n",
    "        # Compute Huber loss\n",
    "        loss = self.criterion(state_action_values, expected_state_action_values) / float(self.num_gamma)\n",
    "        self.model_loss_hist[gamma_idx].append(loss.detach().numpy())\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def select_action(self, gamma_idx):\n",
    "        \"\"\"\n",
    "           Select action before the time horizon change\n",
    "        \"\"\"\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.config.EPS_END + (self.config.EPS_START - self.config.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done[gamma_idx] / self.config.EPS_DECAY)\n",
    "        self.steps_done[gamma_idx] += 1\n",
    "\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                input = torch.cat((self.states[gamma_idx], torch.tensor([self.gammas[gamma_idx]]).unsqueeze(0)), dim=1)\n",
    "                \n",
    "                assert self.states[gamma_idx].shape[0] == input.shape[0]\n",
    "                \n",
    "                z = self.policy_net(input) \n",
    "                action_values = torch.sum((z[:, :, :-1] - z[:, :, 1:]) * self.middle_sensitivities, dim=2) \n",
    "                \n",
    "                assert action_values.shape[0] == self.states[gamma_idx].shape[0]\n",
    "            \n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return action_values.max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[self.envs[gamma_idx].action_space.sample()]], device=self.device, dtype=torch.long)\n",
    "    \n",
    "\n",
    "    def select_action_after_H_change(self, gamma_idx):\n",
    "        \"\"\"\n",
    "           Select action after the time horizon change using the inverse Laplace transform\n",
    "        \"\"\"\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.config.EPS_END + (self.config.EPS_START - self.config.EPS_END) * \\\n",
    "            math.exp(-1. * self.steps_done[gamma_idx] / self.config.EPS_DECAY)\n",
    "        self.steps_done[gamma_idx] += 1\n",
    "\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                input = torch.cat((self.states[gamma_idx], torch.tensor([self.gammas[gamma_idx]]).unsqueeze(0)), dim=1)\n",
    "                \n",
    "                assert self.states[gamma_idx].shape[0] == input.shape[0]\n",
    "                Q_gamma = np.zeros((1, self.action_dim, self.num_gamma_to_tau, self.num_sensitivities))\n",
    "                for idx, gamma in enumerate(self.gammas_to_tau):\n",
    "                    input = torch.cat((self.states[gamma_idx], torch.tensor([gamma]).unsqueeze(0)), dim=1)\n",
    "                    Q_gamma[:, :, idx, :] = self.policy_net(input) \n",
    "                \n",
    "                tau_space = SVD_approximation_inverse_Laplace(self.config, Q_gamma)\n",
    "                \n",
    "                # eq. 13\n",
    "                gammas_pow_tau = torch.tensor([self.gammas[gamma_idx]**tau for tau in range(self.K)])\n",
    "                action_values = torch.matmul(torch.matmul(tau_space, self.middle_sensitivities[:-1]), gammas_pow_tau)\n",
    "                \n",
    "                print(\"Action Values shape after change in time horizon\", action_values.shape)\n",
    "                                    \n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return action_values.max(1).indices.view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[self.envs[gamma_idx].action_space.sample()]], device=self.device, dtype=torch.long)\n",
    "      \n",
    "        \n",
    "    def eval_step(self, render=True):\n",
    "        \"\"\"\n",
    "        Evaluation using the trained target network, no training involved\n",
    "        :param render: whether to visualize the evaluation or not\n",
    "        \"\"\"\n",
    "        self.max_steps = self.initial_number_steps\n",
    "        \n",
    "        for gamma_idx in range(self.gammas):\n",
    "            \n",
    "            for each_ep in range(self.config.evaluate_episodes):\n",
    "                \n",
    "                if self.time_horizon_change is not None and each_ep == self.time_horizon_change:\n",
    "                   self.max_steps = self.later_number_steps\n",
    "                \n",
    "                state, info = self.envs[gamma_idx].reset(seed=self.seed) \n",
    "                state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "\n",
    "                print('Episode: {} Reward: {} Training_Max_Reward: {}'.format(each_ep, self.check_model_improved[0].item(),\n",
    "                                                                                self.best_max[0].item()))\n",
    "                print('-' * 64)\n",
    "                self.check_model_improved = 0\n",
    "\n",
    "                for t in count(): \n",
    "                    if (self.time_horizon_change is None) or (self.time_horizon_change is not None and each_ep < self.time_horizon_change):\n",
    "                        action = self.select_action(gamma_idx)\n",
    "                    else:\n",
    "                        action = self.select_action_after_H_change(gamma_idx) \n",
    "                        \n",
    "                    action = self.select_action(gamma_idx=-1)\n",
    "                    input = torch.cat((state, torch.tensor([self.gammas[gamma_idx]]).unsqueeze(0).repeat(state.shape[0], 1)), dim=1) # NOTE evaluating for the ith gamma\n",
    "                    z = self.policy_net(input)\n",
    "                    observation, reward, terminated, truncated, _ = self.envs[gamma_idx].step(action.item()) \n",
    "                    reward = torch.tensor([reward], device=self.device)\n",
    "                    done = terminated or truncated \n",
    "\n",
    "                    if render:\n",
    "                        self.envs[gamma_idx].render() \n",
    "                    \n",
    "                    self.check_model_improved += reward\n",
    "                \n",
    "                    if done or t == self.max_steps:\n",
    "                        break\n",
    "                    else:\n",
    "                        next_state = torch.tensor(observation, dtype=torch.float32, device=self.device).unsqueeze(0)  \n",
    "                        state = next_state\n",
    "        print('Complete')\n",
    "        \n",
    "        # Calculate the runtime\n",
    "        runtime = self.end_time - self.start_time\n",
    "        print(f'Runtime: {runtime} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gammas :  tensor([0.0100, 0.8199, 0.9035, 0.9341, 0.9500])\n"
     ]
    }
   ],
   "source": [
    "dqn_agent = LaplaceDQNAgentMultgamIntv(game, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 0 Max_Reward: 0\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alpayozkan/opt/anaconda3/envs/distrl_proj/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Reward: 0 Max_Reward: 0\n",
      "----------------------------------------------------------------\n",
      "Episode: 0 Reward: 0 Max_Reward: 0\n",
      "----------------------------------------------------------------\n",
      "Episode: 0 Reward: 0 Max_Reward: 0\n",
      "----------------------------------------------------------------\n",
      "Episode: 0 Reward: 0 Max_Reward: 0\n",
      "----------------------------------------------------------------\n",
      "Episode: 1 Reward: 30.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 1 Reward: 24.0 Max_Reward: 24.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 1 Reward: 25.0 Max_Reward: 25.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 1 Reward: 22.0 Max_Reward: 22.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 1 Reward: 25.0 Max_Reward: 25.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 2 Reward: 28.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 2 Reward: 24.0 Max_Reward: 24.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 2 Reward: 22.0 Max_Reward: 25.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 2 Reward: 15.0 Max_Reward: 22.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 2 Reward: 14.0 Max_Reward: 25.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 3 Reward: 19.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 3 Reward: 18.0 Max_Reward: 24.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 3 Reward: 16.0 Max_Reward: 25.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 3 Reward: 16.0 Max_Reward: 22.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 3 Reward: 16.0 Max_Reward: 25.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 4 Reward: 34.0 Max_Reward: 34.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 4 Reward: 13.0 Max_Reward: 24.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 4 Reward: 40.0 Max_Reward: 40.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 4 Reward: 30.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 4 Reward: 22.0 Max_Reward: 25.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 5 Reward: 28.0 Max_Reward: 34.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 5 Reward: 14.0 Max_Reward: 24.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 5 Reward: 16.0 Max_Reward: 40.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 5 Reward: 28.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 5 Reward: 16.0 Max_Reward: 25.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 6 Reward: 38.0 Max_Reward: 38.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 6 Reward: 12.0 Max_Reward: 24.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 6 Reward: 12.0 Max_Reward: 40.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 6 Reward: 20.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 6 Reward: 30.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 7 Reward: 14.0 Max_Reward: 38.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 7 Reward: 20.0 Max_Reward: 24.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 7 Reward: 21.0 Max_Reward: 40.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 7 Reward: 19.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 7 Reward: 45.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 8 Reward: 11.0 Max_Reward: 38.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 8 Reward: 27.0 Max_Reward: 27.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 8 Reward: 17.0 Max_Reward: 40.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 8 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 8 Reward: 15.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 9 Reward: 42.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 9 Reward: 20.0 Max_Reward: 27.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 9 Reward: 12.0 Max_Reward: 40.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 9 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 9 Reward: 17.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 10 Reward: 10.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 10 Reward: 13.0 Max_Reward: 27.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 10 Reward: 15.0 Max_Reward: 40.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 10 Reward: 15.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 10 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 11 Reward: 17.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 11 Reward: 19.0 Max_Reward: 27.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 11 Reward: 13.0 Max_Reward: 40.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 11 Reward: 16.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 11 Reward: 14.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 12 Reward: 42.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 12 Reward: 10.0 Max_Reward: 27.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 12 Reward: 13.0 Max_Reward: 40.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 12 Reward: 8.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 12 Reward: 8.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 13 Reward: 10.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 13 Reward: 42.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 13 Reward: 47.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 13 Reward: 13.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 13 Reward: 29.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 14 Reward: 13.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 14 Reward: 12.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 14 Reward: 12.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 14 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 14 Reward: 15.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 15 Reward: 10.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 15 Reward: 15.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 15 Reward: 15.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 15 Reward: 9.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 15 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 16 Reward: 15.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 16 Reward: 13.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 16 Reward: 17.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 16 Reward: 13.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 16 Reward: 15.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 17 Reward: 37.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 17 Reward: 11.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 17 Reward: 10.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 17 Reward: 16.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 17 Reward: 10.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 18 Reward: 31.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 18 Reward: 9.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 18 Reward: 13.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 18 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 18 Reward: 12.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 19 Reward: 15.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 19 Reward: 14.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 19 Reward: 16.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 19 Reward: 15.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 19 Reward: 14.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 20 Reward: 16.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 20 Reward: 9.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 20 Reward: 11.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 20 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 20 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 21 Reward: 16.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 21 Reward: 15.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 21 Reward: 10.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 21 Reward: 14.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 21 Reward: 16.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 22 Reward: 23.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 22 Reward: 20.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 22 Reward: 13.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 22 Reward: 9.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 22 Reward: 10.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 23 Reward: 94.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 23 Reward: 15.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 23 Reward: 26.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 23 Reward: 13.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 23 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 24 Reward: 10.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 24 Reward: 16.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 24 Reward: 13.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 24 Reward: 17.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 24 Reward: 19.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 25 Reward: 11.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 25 Reward: 18.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 25 Reward: 12.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 25 Reward: 12.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 25 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 26 Reward: 16.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 26 Reward: 14.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 26 Reward: 13.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 26 Reward: 17.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 26 Reward: 10.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 27 Reward: 13.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 27 Reward: 20.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 27 Reward: 11.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 27 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 27 Reward: 10.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 28 Reward: 10.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 28 Reward: 12.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 28 Reward: 15.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 28 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 28 Reward: 18.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 29 Reward: 11.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 29 Reward: 24.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 29 Reward: 16.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 29 Reward: 23.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 29 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 30 Reward: 33.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 30 Reward: 11.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 30 Reward: 9.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 30 Reward: 8.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 30 Reward: 15.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 31 Reward: 22.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 31 Reward: 10.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 31 Reward: 10.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 31 Reward: 12.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 31 Reward: 9.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 32 Reward: 30.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 32 Reward: 9.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 32 Reward: 9.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 32 Reward: 20.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 32 Reward: 18.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 33 Reward: 19.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 33 Reward: 13.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 33 Reward: 8.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 33 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 33 Reward: 15.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 34 Reward: 68.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 34 Reward: 11.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 34 Reward: 8.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 34 Reward: 22.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 34 Reward: 16.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 35 Reward: 11.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 35 Reward: 11.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 35 Reward: 11.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 35 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 35 Reward: 13.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 36 Reward: 15.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 36 Reward: 18.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 36 Reward: 13.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 36 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 36 Reward: 12.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 37 Reward: 30.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 37 Reward: 10.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 37 Reward: 12.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 37 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 37 Reward: 8.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 38 Reward: 14.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 38 Reward: 8.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 38 Reward: 9.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 38 Reward: 12.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 38 Reward: 16.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 39 Reward: 38.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 39 Reward: 13.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 39 Reward: 8.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 39 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 39 Reward: 8.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 40 Reward: 41.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 40 Reward: 8.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 40 Reward: 9.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 40 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 40 Reward: 8.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 41 Reward: 11.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 41 Reward: 15.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 41 Reward: 15.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 41 Reward: 13.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 41 Reward: 9.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 42 Reward: 14.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 42 Reward: 13.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 42 Reward: 29.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 42 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 42 Reward: 13.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 43 Reward: 13.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 43 Reward: 11.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 43 Reward: 12.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 43 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 43 Reward: 10.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 44 Reward: 10.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 44 Reward: 9.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 44 Reward: 16.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 44 Reward: 16.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 44 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 45 Reward: 12.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 45 Reward: 12.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 45 Reward: 10.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 45 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 45 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 46 Reward: 27.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 46 Reward: 13.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 46 Reward: 10.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 46 Reward: 14.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 46 Reward: 13.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 47 Reward: 11.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 47 Reward: 10.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 47 Reward: 12.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 47 Reward: 12.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 47 Reward: 8.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 48 Reward: 35.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 48 Reward: 9.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 48 Reward: 10.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 48 Reward: 9.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 48 Reward: 12.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 49 Reward: 27.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 49 Reward: 12.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 49 Reward: 12.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 49 Reward: 12.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 49 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 50 Reward: 19.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 50 Reward: 14.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 50 Reward: 10.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 50 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 50 Reward: 12.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 51 Reward: 12.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 51 Reward: 13.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 51 Reward: 8.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 51 Reward: 9.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 51 Reward: 10.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 52 Reward: 11.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 52 Reward: 11.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 52 Reward: 13.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 52 Reward: 12.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 52 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 53 Reward: 11.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 53 Reward: 11.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 53 Reward: 11.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 53 Reward: 12.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 53 Reward: 12.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 54 Reward: 13.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 54 Reward: 9.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 54 Reward: 10.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 54 Reward: 8.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 54 Reward: 10.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 55 Reward: 13.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 55 Reward: 11.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 55 Reward: 9.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 55 Reward: 12.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 55 Reward: 8.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 56 Reward: 72.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 56 Reward: 12.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 56 Reward: 11.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 56 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 56 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 57 Reward: 31.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 57 Reward: 10.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 57 Reward: 14.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 57 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 57 Reward: 17.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 58 Reward: 10.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 58 Reward: 11.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 58 Reward: 11.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 58 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 58 Reward: 17.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 59 Reward: 16.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 59 Reward: 18.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 59 Reward: 11.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 59 Reward: 12.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 59 Reward: 13.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 60 Reward: 11.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 60 Reward: 13.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 60 Reward: 18.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 60 Reward: 14.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 60 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 61 Reward: 22.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 61 Reward: 21.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 61 Reward: 14.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 61 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 61 Reward: 10.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 62 Reward: 15.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 62 Reward: 22.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 62 Reward: 14.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 62 Reward: 14.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 62 Reward: 19.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 63 Reward: 10.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 63 Reward: 14.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 63 Reward: 10.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 63 Reward: 16.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 63 Reward: 11.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 64 Reward: 10.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 64 Reward: 13.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 64 Reward: 10.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 64 Reward: 10.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 64 Reward: 18.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 65 Reward: 11.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 65 Reward: 18.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 65 Reward: 12.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 65 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 65 Reward: 17.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 66 Reward: 14.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 66 Reward: 11.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 66 Reward: 13.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 66 Reward: 14.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 66 Reward: 10.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 67 Reward: 11.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 67 Reward: 13.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 67 Reward: 16.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 67 Reward: 16.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 67 Reward: 12.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 68 Reward: 15.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 68 Reward: 10.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 68 Reward: 12.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 68 Reward: 13.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 68 Reward: 10.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 69 Reward: 11.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 69 Reward: 16.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 69 Reward: 18.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 69 Reward: 11.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 69 Reward: 16.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 70 Reward: 12.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 70 Reward: 12.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 70 Reward: 12.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 70 Reward: 13.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 70 Reward: 17.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 71 Reward: 14.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 71 Reward: 13.0 Max_Reward: 42.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 71 Reward: 12.0 Max_Reward: 47.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 71 Reward: 13.0 Max_Reward: 30.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 71 Reward: 27.0 Max_Reward: 45.0\n",
      "----------------------------------------------------------------\n",
      "Episode: 72 Reward: 13.0 Max_Reward: 94.0\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransition\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 119\u001b[0m, in \u001b[0;36mLaplaceDQNAgentMultgamIntv.transition\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs[gamma_idx]\n\u001b[1;32m    117\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[gamma_idx]\n\u001b[0;32m--> 119\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgamma_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# No horizon change during training for now\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# if (self.time_horizon_change is None) or (self.time_horizon_change is not None and i_episode < self.time_horizon_change):\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#     action = self.select_action(gamma_idx)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m#     action = self.select_action_after_H_change(gamma_idx) \u001b[39;00m\n\u001b[1;32m    126\u001b[0m observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[8], line 275\u001b[0m, in \u001b[0;36mLaplaceDQNAgentMultgamIntv.select_action\u001b[0;34m(self, gamma_idx)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[gamma_idx], torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgammas[gamma_idx]])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[gamma_idx]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 275\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[1;32m    276\u001b[0m action_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((z[:, :, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m z[:, :, \u001b[38;5;241m1\u001b[39m:]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_sensitivities, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m action_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates[gamma_idx]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/distrl_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/distrl_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 50\u001b[0m, in \u001b[0;36mDQNNetMultgamInv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m res_tens[med_indexes] \u001b[38;5;241m=\u001b[39m med_out\n\u001b[1;32m     48\u001b[0m res_tens[high_indexes] \u001b[38;5;241m=\u001b[39m high_out\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlow_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/distrl_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/distrl_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mDQNNetMultgam.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x))\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreshape_layer(x)\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcumsum(x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/distrl_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/distrl_proj/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/distrl_proj/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dqn_agent.transition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = torch.rand(torch.Size([128, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(61)\n",
      "tensor(30)\n",
      "tensor(37)\n",
      "tensor(128)\n"
     ]
    }
   ],
   "source": [
    "# take gammas\n",
    "gamm_tens = tmp[:,-1]\n",
    "low_gam_mask = gamm_tens < 0.5\n",
    "med_gam_mask = (gamm_tens >= 0.5) &  (gamm_tens < 0.75)\n",
    "high_gam_mask = (gamm_tens >= 0.75) &  (gamm_tens <= 1)\n",
    "\n",
    "print(low_gam_mask.sum())\n",
    "print(med_gam_mask.sum())\n",
    "print(high_gam_mask.sum())\n",
    "print(low_gam_mask.sum() + med_gam_mask.sum() + high_gam_mask.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tens = torch.zeros_like(tmp)\n",
    "res_tens[low_gam_mask] = tmp[low_gam_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False,  True,  True, False, False, False, False,  True, False,\n",
       "         True, False, False,  True, False, False, False, False, False, False,\n",
       "         True,  True, False,  True,  True,  True,  True, False,  True, False,\n",
       "         True, False,  True, False, False, False,  True, False, False,  True,\n",
       "        False,  True, False, False,  True,  True, False,  True, False, False,\n",
       "        False, False, False, False, False,  True,  True, False,  True, False,\n",
       "         True,  True,  True, False,  True,  True,  True, False, False,  True,\n",
       "         True,  True,  True, False,  True,  True, False,  True, False,  True,\n",
       "        False, False,  True, False, False,  True,  True, False,  True, False,\n",
       "        False, False,  True,  True, False,  True,  True,  True, False,  True,\n",
       "         True,  True, False,  True, False, False, False,  True,  True,  True,\n",
       "        False, False,  True, False,  True,  True, False,  True,  True, False,\n",
       "        False, False, False,  True, False,  True,  True, False])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "low_gam_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8748, 0.5193, 0.2856, 0.4319, 0.1086],\n",
       "        [0.8229, 0.2084, 0.4469, 0.1512, 0.0099],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8206, 0.9857, 0.0246, 0.2126, 0.3563],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8209, 0.3278, 0.0333, 0.2286, 0.2230],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2277, 0.5540, 0.0529, 0.8159, 0.2664],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8974, 0.1090, 0.9185, 0.6900, 0.2447],\n",
       "        [0.2338, 0.2390, 0.7792, 0.4163, 0.2683],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6958, 0.3885, 0.1357, 0.1395, 0.0680],\n",
       "        [0.6524, 0.1487, 0.9254, 0.3585, 0.2363],\n",
       "        [0.3155, 0.3640, 0.9653, 0.1856, 0.1922],\n",
       "        [0.5777, 0.6392, 0.3328, 0.5419, 0.2656],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2234, 0.1516, 0.2672, 0.9821, 0.4983],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5400, 0.7833, 0.8723, 0.1079, 0.3001],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5569, 0.4816, 0.4202, 0.5382, 0.3251],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7786, 0.6397, 0.4535, 0.7399, 0.0554],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4609, 0.3143, 0.7280, 0.8206, 0.2192],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4380, 0.9178, 0.8775, 0.6590, 0.1069],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3628, 0.1384, 0.4916, 0.6648, 0.4771],\n",
       "        [0.3804, 0.8830, 0.5410, 0.0474, 0.1662],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5158, 0.9807, 0.0896, 0.4352, 0.4581],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3744, 0.0448, 0.7981, 0.9020, 0.0199],\n",
       "        [0.5193, 0.8507, 0.6099, 0.3700, 0.0430],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0753, 0.5994, 0.4134, 0.2062, 0.0292],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7774, 0.9600, 0.3536, 0.0338, 0.0393],\n",
       "        [0.6151, 0.9070, 0.9174, 0.7684, 0.3449],\n",
       "        [0.3832, 0.6465, 0.7629, 0.8131, 0.1426],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0362, 0.8969, 0.5992, 0.1583, 0.1264],\n",
       "        [0.4178, 0.2010, 0.9871, 0.7947, 0.3881],\n",
       "        [0.2287, 0.2472, 0.9479, 0.6830, 0.0039],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8586, 0.1474, 0.6648, 0.2270, 0.0604],\n",
       "        [0.3164, 0.7995, 0.5064, 0.3707, 0.4125],\n",
       "        [0.0959, 0.7381, 0.3174, 0.0369, 0.0310],\n",
       "        [0.1366, 0.4698, 0.8120, 0.7372, 0.4850],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3394, 0.6439, 0.7019, 0.0657, 0.3828],\n",
       "        [0.3167, 0.7148, 0.5918, 0.9336, 0.1761],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1715, 0.6955, 0.1048, 0.3008, 0.4734],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5589, 0.5443, 0.7216, 0.5686, 0.2372],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6797, 0.4968, 0.2777, 0.6647, 0.1334],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.8820, 0.3623, 0.7734, 0.7108, 0.0644],\n",
       "        [0.9390, 0.2775, 0.8032, 0.0647, 0.4893],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2538, 0.5410, 0.3446, 0.5028, 0.3529],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5762, 0.6738, 0.7512, 0.9270, 0.0234],\n",
       "        [0.8506, 0.9464, 0.5737, 0.6974, 0.1506],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0444, 0.5242, 0.7650, 0.8339, 0.0169],\n",
       "        [0.3957, 0.3967, 0.0871, 0.3612, 0.4553],\n",
       "        [0.8722, 0.2352, 0.7486, 0.2985, 0.2180],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.9299, 0.4938, 0.5199, 0.4824, 0.3031],\n",
       "        [0.9139, 0.6619, 0.5090, 0.2765, 0.3799],\n",
       "        [0.3005, 0.0556, 0.1560, 0.5661, 0.0399],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0064, 0.0948, 0.9686, 0.2121, 0.1728],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5865, 0.9879, 0.6381, 0.6276, 0.0784],\n",
       "        [0.8057, 0.0060, 0.1918, 0.9017, 0.1216],\n",
       "        [0.2500, 0.9962, 0.8107, 0.4552, 0.4385],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6285, 0.3323, 0.3961, 0.9035, 0.0108],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6842, 0.0363, 0.4987, 0.6056, 0.0833],\n",
       "        [0.3791, 0.7716, 0.0759, 0.3757, 0.3201],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0664, 0.0113, 0.0501, 0.3337, 0.2969],\n",
       "        [0.0336, 0.6819, 0.6491, 0.4080, 0.2500],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0159, 0.6607, 0.1783, 0.2572, 0.0603],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2575, 0.9167, 0.9970, 0.5943, 0.2610],\n",
       "        [0.2420, 0.2747, 0.9505, 0.3804, 0.3347],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distrl_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
